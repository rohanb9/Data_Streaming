How did changing values on the SparkSession property parameters affect the throughput and latency of the data?
We can monitor processedRowsPerSecond from the Progress Report.
By changing SparkSession property parameters, it shows changes in processedRowsPerSecond. It's got increased or decreased based on parameters.
It is basically the number of rows processed per second. Higher the number which means higher throughput.

What were the 2-3 most efficient SparkSession property key/value pairs? Through testing multiple variations on values, how can you tell these were the most optimal?
->Most efficient SparkSession property:
	1)spark.sql.shuffle.partitions
		spark.sql.shuffle.partitions configure the number of partitions that are used when shuffling data for joins or aggregations.
		To change the settings we can do it as:
			sqlContext.setConf("spark.sql.shuffle.partitions", "300")

	2)spark.default.parallelism
		spark.default.parallelism is the default number of partitions in RDDs returned by transformations like join, reduceByKey, and parallelize when not set explicitly by the user.spark.default.parallelism seems to only be working for raw RDD and is ignored when working with dataframes.
		To change the settings we can do it as:
			sqlContext.setConf("spark.default.parallelism", "300")

	3)spark.streaming.kafka.maxRatePerPartition:
		For rate-limiting, we can use the Spark configuration variable spark.streaming.kafka.maxRatePerPartition to set the maximum number of messages per partition per batch.
		it prevents from being overwhelmed when there is a large number of unprocessed messages.

->To increase parallelism, we need to increase the number of partitions in Kafka topic also we need to increase the number of consumers in the consumer group. If we increase both of them we can process topic data faster.
Also, We need to consider the number of cores and machines currently used for running jobs while increasing parallelism.

We can check the Progress Report for our job. Some of the important params are:
numInputRows : The aggregate (across all sources) number of records processed in a trigger.
inputRowsPerSecond : The aggregate (across all sources) rate of data arriving.
processedRowsPerSecond : The aggregate (across all sources) rate at which Spark is processing data.
